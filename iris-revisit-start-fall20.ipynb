{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A \"Hello World\" Example of Machine Learning - Revisit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Iris dataset from scikit-learn. \n",
    "\n",
    "The first column represents Sepal length, the second column represents Sepal width,  the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes (type of species) are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica.\n",
    "\n",
    "Here, we are using only two features: the third and fourth columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "#iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, [2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Class labels: [0 1 2]\n"
    }
   ],
   "source": [
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn algorithms support multi-class classification via the One-Versus-Rest(OvR) method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into 70% training and 30% test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Labels counts in y: [50 50 50]\nLabels counts in y_train: [35 35 35]\nLabels counts in y_test: [15 15 15]\n"
    }
   ],
   "source": [
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler() #center the distribution around zero (mean), with a standard deviation of 1.\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=0.1,\n           fit_intercept=True, max_iter=100, n_iter_no_change=5, n_jobs=None,\n           penalty=None, random_state=42, shuffle=True, tol=0.001,\n           validation_fraction=0.1, verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 235
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(max_iter=100, eta0=0.1, random_state=42)\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model with the hold-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Misclassified samples: 2\n"
    }
   ],
   "source": [
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified samples: ' + str((y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.9555555555555556\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 2, 1])"
     },
     "metadata": {},
     "execution_count": 238
    }
   ],
   "source": [
    "X_new = [[1.1, 0.2],[0.4, 1.9], [1.4, 0.2]]\n",
    "y_new = ppn.predict(X_new)\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.88888889, 0.48148148, 0.7037037 , 0.95833333])"
     },
     "metadata": {},
     "execution_count": 239
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(ppn, X_train_std, y_train, cv=4, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-features: sccuracy score: array([0.88888889, 0.48148148, 0.7037037 , 0.95833333])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Use all four features to train the model and use cross validaton to check if the results better? Briefly explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "#iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Class labels: [0 1 2]\n"
    }
   ],
   "source": [
    "b = iris.target\n",
    "print('Class labels:', np.unique(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "A_train, A_test, b_train, b_test = train_test_split(\n",
    "    A, b, test_size=0.3, random_state=1, stratify=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Labels counts in b: [50 50 50]\nLabels counts in b_train: [35 35 35]\nLabels counts in b_test: [15 15 15]\n"
    }
   ],
   "source": [
    "print('Labels counts in b:', np.bincount(y))\n",
    "print('Labels counts in b_train:', np.bincount(b_train))\n",
    "print('Labels counts in b_test:', np.bincount(b_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler() #center the distribution around zero (mean), with a standard deviation of 1.\n",
    "sc.fit(A_train)\n",
    "A_train_std = sc.transform(A_train)\n",
    "A_test_std = sc.transform(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=0.1,\n           fit_intercept=True, max_iter=100, n_iter_no_change=5, n_jobs=None,\n           penalty=None, random_state=42, shuffle=True, tol=0.001,\n           validation_fraction=0.1, verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 248
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(max_iter=100, eta0=0.1, random_state=42)\n",
    "ppn.fit(A_train_std, b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Misclassified samples: 3\n"
    }
   ],
   "source": [
    "b_pred = ppn.predict(A_test_std)\n",
    "print('Misclassified samples: ' + str((b_test != b_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.9333333333333333\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ' + str(accuracy_score(b_test, b_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.92592593, 0.77777778, 0.66666667, 0.875     ])"
     },
     "metadata": {},
     "execution_count": 251
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(ppn, A_train_std, b_train, cv=4, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all four features caused the acurracy rate to drop from 96% with two features to 93% with four. This is most likely because the two features are more linearly separable. Once you add the the other two features it becomes less perfectly linearly separable and so we misclassify more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Try with the scikit-learn stochastic gradient descent model instead of perceptron. Use all four features. Evaluate with cross-validation how does the model perform in terms of accuracy using both two features and four features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = iris.data[:, [2, 3]]\n",
    "t = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "S_train, S_test, t_train, t_test = train_test_split(\n",
    "    S, t, test_size=0.3, random_state=1, stratify=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler() #center the distribution around zero (mean), with a standard deviation of 1.\n",
    "sc.fit(S_train)\n",
    "S_train_std = sc.transform(S_train)\n",
    "S_test_std = sc.transform(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n              validation_fraction=0.1, verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 255
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "sgd.fit(S_train_std, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Misclassified samples: 0\n"
    }
   ],
   "source": [
    "t_pred = sgd.predict(S_test_std)\n",
    "print('Misclassified samples: ' + str((t_test != t_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 1.0\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ' + str(accuracy_score(t_test, t_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### four features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Class labels: [0 1 2]\n"
    }
   ],
   "source": [
    "C = iris.data\n",
    "d = iris.target\n",
    "print('Class labels:', np.unique(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "C_train, C_test, d_train, d_test = train_test_split(\n",
    "    C, d, test_size=0.3, random_state=1, stratify=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler() #center the distribution around zero (mean), with a standard deviation of 1.\n",
    "sc.fit(C_train)\n",
    "C_train_std = sc.transform(C_train)\n",
    "C_test_std = sc.transform(C_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n              power_t=0.5, random_state=42, shuffle=True, tol=0.001,\n              validation_fraction=0.1, verbose=0, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 261
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "sgd.fit(C_train_std, d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Misclassified samples: 5\n"
    }
   ],
   "source": [
    "d_pred = sgd.predict(C_test_std)\n",
    "print('Misclassified samples: ' + str((d_test != d_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Accuracy: 0.8888888888888888\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy: ' + str(accuracy_score(d_test, d_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic gradient descent model performed better on the two features similar to the perceptron model. With four features it performed worse and misclassified 5 samples as opposed to next to nothing with two features. The two features that we use for our two feature algorithms appear more linearly separable and thus we can predict better. When we use four features it is less linearly separable and thus we perform worse. Also the sotchastic gradient descent algorithm randomly picks a data point to begin with. With more data provided by the extra features,we have more random options for the model and also more when the model updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python_defaultSpec_1600539563862"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}